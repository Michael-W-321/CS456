{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6a989ea-3016-44cf-9c6d-a1d2a0770407",
   "metadata": {},
   "source": [
    "Lab 2 - Data Reading and Processing\n",
    "Name: Michael"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "099fac1e-c8bb-4551-9641-0ea4daebb20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing needed functions\n",
    "import os\n",
    "import re \n",
    "import json\n",
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "000cb199-2414-4f79-8b36-ffcb8ef2d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean numeric text to floats\n",
    "#handling commas, parentheses for negatives\n",
    "def parse_money(x):\n",
    "    \"\"\"converting various strings to floats which returns np.nan when failed\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    #convert to string and strip the whitespace \n",
    "    s = str(x).strip()\n",
    "    if s == '' or s.upper() in {\"N.A.\", \"NA\", \"N/A\", \"Null\"}:\n",
    "        return np.nan\n",
    "#treating the parentheses as negative numbers\n",
    "#removing stray characters\n",
    "#replacing missreads\n",
    "    s = s.replace('$','') \n",
    "    s = s.replace(',','') \n",
    "    s = s.replace('\\u2009','') \n",
    "\n",
    "    if re.search(r\"\\d\", s) and re.search(r\"[Oo]\", s):\n",
    "        s = re.sub(r\"[Oo]\", '0', s)\n",
    "#handleing parentheses \n",
    "    neg = False\n",
    "    if s.startswith('(') and s.endswith(')'):\n",
    "        neg = True\n",
    "        s = s[1:-1]\n",
    "#removing any remaming non numerics & non dots & non minus characters\n",
    "    s = re.sub(r\"[^0-9.\\-]\", '', s)\n",
    "    if s in ['', '.', '-', '-.']:\n",
    "        return np.nan\n",
    "    try:\n",
    "        v = float(s)\n",
    "        return -v if neg else v\n",
    "    except Exception:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b1932c0-2aab-49b5-bf59-ff116016f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read fortune500.csv\n",
    "fortune_path = 'fortune500.csv'\n",
    "if os.path.exists(fortune_path):\n",
    "#trying to read with pandas allowing bad lines\n",
    "    try:\n",
    "        df_csv = pd.read_csv(fortune_path, engine='python', on_bad_lines='skip', dtype=str)\n",
    "    except Exception:\n",
    "#reading line-by-line and attempting a manual split on commas while saving quoted parts\n",
    "        lines = open(fortune_path, 'r', encoding='utf-8', errors='ignore').read().splitlines()\n",
    "        parsed = []\n",
    "        for line in lines:\n",
    "#naive split if JSON-like line, try to parse as JSON\n",
    "            parsed.append(line.split(','))\n",
    "        df_csv = pd.DataFrame(parsed[1:], columns=parsed[0])\n",
    "else:\n",
    "    df_csv = pd.DataFrame() # empty placeholder if file missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "517be8a6-f169-4f98-93df-a6d579af9f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read lines.json\n",
    "lines_path = 'lines.json'\n",
    "#will hold parsed json objects\n",
    "json_rows = []\n",
    "if os.path.exists(lines_path):\n",
    "    with open(lines_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for raw in f:\n",
    "            raw = raw.strip()\n",
    "            if not raw:\n",
    "                continue\n",
    "            try:\n",
    "#many files are like JSON objects per line\n",
    "                obj = json.loads(raw)\n",
    "                json_rows.append(obj)\n",
    "            except Exception:\n",
    "#trying to salvage by replacing trailing commas or fixing common mistakes\n",
    "                try:\n",
    "#remove trailing commas before the closing brace\n",
    "                    cleaned = re.sub(r',\\s*}', '}', raw)\n",
    "                    obj = json.loads(cleaned)\n",
    "                    json_rows.append(obj)\n",
    "                except Exception:\n",
    "#giving up on this line\n",
    "                    continue\n",
    "    df_json = pd.DataFrame(json_rows)\n",
    "else:\n",
    "    df_json = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ed72229-18d5-488b-a36b-846487b82889",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read unstructureddata (1).txt\n",
    "unstructured_path = 'unstructureddata (1).txt'\n",
    "unstructured_rows = []\n",
    "if os.path.exists(unstructured_path):\n",
    "    text = open(unstructured_path, 'r', encoding='utf-8', errors='ignore').read()\n",
    "#trying to extract JSON-like or CSV-like lines\n",
    "#lines containing Year and Company and Revenue and Profit keys\n",
    "    candidate_lines = re.findall(r\"\\{[^\\}]*\\}\", text) # find {...} blocks\n",
    "    if candidate_lines:\n",
    "        for block in candidate_lines:\n",
    "            try:\n",
    "                obj = json.loads(block)\n",
    "                unstructured_rows.append(obj)\n",
    "            except Exception:\n",
    "#cleanup and try a trivial key - value extraction\n",
    "                kvs = re.findall(r'\"?([A-Za-z ]+)\"?\\s*:\\s*\"?([^,\\}\\n]+)\"?', block)\n",
    "                d = {k.strip(): v.strip() for k, v in kvs}\n",
    "            if d:\n",
    "                unstructured_rows.append(d)\n",
    "    else:\n",
    "#each line might be a CSV or whitespace separated. Try splitting by newline and colon/comma\n",
    "        for line in text.splitlines():\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "#try to split on comma into four or five parts\n",
    "            parts = [p.strip() for p in re.split(r',\\s*(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)', line) if p.strip()]\n",
    "            if len(parts) >= 3:\n",
    "#heuristically assign the fields\n",
    "#ex: Year, Rank, Company, Revenue, Profit\n",
    "                d = {}\n",
    "                if re.match(r'^\\d{4}$', parts[0]):\n",
    "                    d['Year'] = parts[0]\n",
    "                    if len(parts) >= 5:\n",
    "                        d['Rank'] = parts[1]\n",
    "                        d['Company'] = parts[2]\n",
    "                        d['Revenue (in millions)'] = parts[3]\n",
    "                        d['Profit (in millions)'] = parts[4]\n",
    "                    elif len(parts) == 4:\n",
    "                        d['Company'] = parts[1]\n",
    "                        d['Revenue (in millions)'] = parts[2]\n",
    "                        d['Profit (in millions)'] = parts[3]\n",
    "                else:\n",
    "# try to find a year anywhere\n",
    "                    y = re.search(r'(19|20)\\d{2}', line)\n",
    "                    if y:\n",
    "                        d['Year'] = y.group(0)\n",
    "#attempt to pull numbers for revenue/profit from end of line\n",
    "                        nums = re.findall(r'[-()\\d,\\.O]+', line)\n",
    "                        if len(nums) >= 2:\n",
    "                            d['Revenue (in millions)'] = nums[-2]\n",
    "                            d['Profit (in millions)'] = nums[-1]\n",
    "#company name - substring between year and first number\n",
    "                        m = re.search(rf\"{d['Year']}\\D+(.+?)\\s+{re.escape(nums[-2])}\", line) if nums else None\n",
    "                        if m:\n",
    "                            d['Company'] = m.group(1).strip()\n",
    "                    if d:\n",
    "                        unstructured_rows.append(d)\n",
    "    df_unstructured = pd.DataFrame(unstructured_rows)\n",
    "else:\n",
    "    df_unstructured = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3f92c2e-4929-4681-bad5-bd0f97f169dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize and standardize column names and union the dataframes\n",
    "#helper to standardize a dataframe's columns to- Year, Rank, Company, Revenue, Profit\n",
    "def standardize_df(df):\n",
    "    \"\"\"Map common variants of column names to a chosen canonical set and return a cleaned copy.\"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df = df.copy()\n",
    "#lower-case columns for mapping\n",
    "    colmap = {}\n",
    "    for c in df.columns:\n",
    "        lc = c.strip().lower()\n",
    "        if 'year' in lc:\n",
    "            colmap[c] = 'Year'\n",
    "        elif 'rank' in lc:\n",
    "            colmap[c] = 'Rank'\n",
    "        elif 'company' in lc:\n",
    "            colmap[c] = 'Company'\n",
    "        elif 'revenue' in lc:\n",
    "            colmap[c] = 'Revenue'\n",
    "        elif 'profit' in lc:\n",
    "            colmap[c] = 'Profit'\n",
    "        else:\n",
    "#save unknown columns as-is\n",
    "            colmap[c] = c\n",
    "    df.rename(columns=colmap, inplace=True)\n",
    "#ensure columns exist\n",
    "    for needed in ['Year', 'Rank', 'Company', 'Revenue', 'Profit']:\n",
    "        if needed not in df.columns:\n",
    "            df[needed] = np.nan\n",
    "#strip whitespace from the company names\n",
    "    df['Company'] = df['Company'].astype(str).str.strip().replace({'nan': np.nan})\n",
    "#clean revenue and profit columns using parse_money\n",
    "    df['Revenue'] = df['Revenue'].apply(parse_money)\n",
    "    df['Profit'] = df['Profit'].apply(parse_money)\n",
    "#year and Rank to numeric when possible\n",
    "    df['Year'] = pd.to_numeric(df['Year'], errors='coerce')\n",
    "    df['Rank'] = pd.to_numeric(df['Rank'], errors='coerce')\n",
    "    return df[['Year', 'Rank', 'Company', 'Revenue', 'Profit']]\n",
    "\n",
    "\n",
    "#standardize available dataframes\n",
    "std_csv = standardize_df(df_csv)\n",
    "std_json = standardize_df(df_json)\n",
    "std_un = standardize_df(df_unstructured)\n",
    "\n",
    "\n",
    "#combine them vertically\n",
    "combined = pd.concat([std_csv, std_json, std_un], ignore_index=True, sort=False)\n",
    "\n",
    "\n",
    "#drop exact duplicate rows\n",
    "combined_before = combined.shape[0]\n",
    "combined.drop_duplicates(inplace=True)\n",
    "combined_after = combined.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a5518da-8bec-43d9-96fc-a44315967693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#further cleaning and standardization\n",
    "#remove rows that have neither company nor year\n",
    "combined = combined[~(combined['Company'].isna() & combined['Year'].isna())].copy()\n",
    "\n",
    "\n",
    "#trim company names and normalize some obvious variants\n",
    "combined['Company'] = combined['Company'].astype(str).str.strip()\n",
    "#more normalizaiton\n",
    "company_normalization = {\n",
    "    'exxon mobil': 'Exxon Mobil',\n",
    "    'chevrontexaco': 'ChevronTexaco',\n",
    "    'intl. business machines': 'International Business Machines',\n",
    "}\n",
    "combined['Company_norm'] = combined['Company'].str.lower().map(lambda s: company_normalization.get(s, None))\n",
    "combined['Company'] = combined.apply(lambda r: r['Company_norm'] if pd.notna(r['Company_norm']) else r['Company'], axis=1)\n",
    "combined.drop(columns=['Company_norm'], inplace=True)\n",
    "\n",
    "\n",
    "#remove rows where revenue and profit are both NaN and Company is NaN\n",
    "combined = combined[~(combined['Company'].isna() & combined['Revenue'].isna() & combined['Profit'].isna())]\n",
    "\n",
    "\n",
    "#save the cleaned combined dataset to CSV\n",
    "combined.to_csv('processed_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "164d53e4-8b8e-4d74-9358-fa7ef8712f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#good data - rows where Year, Company, Revenue, Profit are all present (non-NaN)\n",
    "good_mask = combined['Year'].notna() & combined['Company'].notna() & combined['Revenue'].notna() & combined['Profit'].notna()\n",
    "good_data_count = int(good_mask.sum())\n",
    "#bad data - rows where any of the needed fields missing\n",
    "bad_data_count = int((~good_mask).sum())\n",
    "#number of unique companies based on the non-null company names\n",
    "unique_companies = int(combined['Company'].dropna().nunique())\n",
    "\n",
    "#determining the company with highest revenue from 1995 to 1998\n",
    "start_year = 1995\n",
    "end_year = 1998\n",
    "#check available years\n",
    "available_years = combined['Year'].dropna().unique()\n",
    "if any((available_years >= start_year) & (available_years <= end_year)):\n",
    "#filter to the target range\n",
    "    sub = combined[(combined['Year'] >= start_year) & (combined['Year'] <= end_year)].copy()\n",
    "#group by company and sum revenue & profit across the years\n",
    "    grp = sub.groupby('Company').agg({'Revenue':'sum','Profit':'sum'}).reset_index()\n",
    "    if not grp.empty:\n",
    "        top_rev_row = grp.loc[grp['Revenue'].idxmax()]\n",
    "        top_profit_row = grp.loc[grp['Profit'].idxmax()]\n",
    "        top_revenue_company = top_rev_row['Company']\n",
    "        top_revenue_value = float(top_rev_row['Revenue'])\n",
    "        top_profit_company = top_profit_row['Company']\n",
    "        top_profit_value = float(top_profit_row['Profit'])\n",
    "    else:\n",
    "        top_revenue_company = 'No data in 1995-1998'\n",
    "        top_revenue_value = np.nan\n",
    "        top_profit_company = 'No data in 1995-1998'\n",
    "        top_profit_value = np.nan\n",
    "    year_note = f\"Used range {start_year}-{end_year}.\"\n",
    "else:\n",
    "#use the available year range and compute top companies across all available years\n",
    "    min_y = int(np.nanmin(available_years)) if len(available_years)>0 else None\n",
    "    max_y = int(np.nanmax(available_years)) if len(available_years)>0 else None\n",
    "    if min_y is None:\n",
    "        top_revenue_company = 'No year data available'\n",
    "        top_revenue_value = np.nan\n",
    "        top_profit_company = 'No year data available'\n",
    "        top_profit_value = np.nan\n",
    "        year_note = 'No years available in dataset.'\n",
    "    else:\n",
    "        sub = combined[(combined['Year'] >= min_y) & (combined['Year'] <= max_y)].copy()\n",
    "        grp = sub.groupby('Company').agg({'Revenue':'sum','Profit':'sum'}).reset_index()\n",
    "        if not grp.empty:\n",
    "            top_rev_row = grp.loc[grp['Revenue'].idxmax()]\n",
    "            top_profit_row = grp.loc[grp['Profit'].idxmax()]\n",
    "            top_revenue_company = top_rev_row['Company']\n",
    "            top_revenue_value = float(top_rev_row['Revenue'])\n",
    "            top_profit_company = top_profit_row['Company']\n",
    "            top_profit_value = float(top_profit_row['Profit'])\n",
    "        else:\n",
    "            top_revenue_company = 'No revenue data'\n",
    "            top_revenue_value = np.nan\n",
    "            top_profit_company = 'No profit data'\n",
    "            top_profit_value = np.nan\n",
    "            year_note = f'1995-1998 not present; used available range {min_y}-{max_y}.'\n",
    "\n",
    "#Results_Combine dataframe\n",
    "results = {\n",
    "    'Metric': [\n",
    "        'Total Good Rows (complete Year, Company, Revenue, Profit)',\n",
    "        'Total Bad Rows (incomplete)',\n",
    "        'Unique Companies (non-null names)',\n",
    "        'Top Revenue Company (1995-1998)',\n",
    "        'Top Revenue Value (in millions)',\n",
    "        'Top Profit Company (1995-1998)',\n",
    "        'Top Profit Value (in millions)',\n",
    "        'Year Note'\n",
    "    ],\n",
    "    'Value': [\n",
    "        good_data_count,\n",
    "        bad_data_count,\n",
    "        unique_companies,\n",
    "        top_revenue_company,\n",
    "        top_revenue_value,\n",
    "        top_profit_company,\n",
    "        top_profit_value,\n",
    "        year_note\n",
    "    ]\n",
    "}\n",
    "Results_Combine = pd.DataFrame(results).set_index('Metric')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37ebe7f1-eaec-4959-98b7-beb8a7197883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                    Value\n",
      "Metric                                                                   \n",
      "Total Good Rows (complete Year, Company, Revenu...                  29011\n",
      "Total Bad Rows (incomplete)                                           393\n",
      "Unique Companies (non-null names)                                    2599\n",
      "Top Revenue Company (1995-1998)                            General Motors\n",
      "Top Revenue Value (in millions)                                  670322.8\n",
      "Top Profit Company (1995-1998)                                Exxon Mobil\n",
      "Top Profit Value (in millions)                                    27540.0\n",
      "Year Note                                           Used range 1995-1998.\n"
     ]
    }
   ],
   "source": [
    "#print Results_Combine\n",
    "print(Results_Combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64014228-bb07-4345-8c85-0554f216a982",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save Results_Combine to CSV\n",
    "Results_Combine.to_csv('Results_Combine.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
